# -*- coding: utf-8 -*-
"""WQD7003 Crop Yield Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqGPrhVT4KCNmuXpK1DZTCcSYEC3NU1D

**Group 16**

Title: Crop Yield Prediction Using Machine Learning Algorithm

*   NUR SHAFIQAH BINTI MOHAMAD JOHARI (22119564​)
*   LAW JIA JIN (22071390)
*   LIM SZE SING (22109557)
*   GAN JING WEN (22065433)

# **1. Business Understanding**

## **1.1 Background**

The latest Hunger Hotspots report by Food and Agriculture Organization (FAO) and World Food Programme (WFP) identifies 18 hotspots (including 22 countries and territories) that will require urgent action between November 2023 and April 2024.

Burkina Faso, Mali, Palestine, South Sudan, and Sudan are hunger hotspots of the highest concern, with significant numbers of people at risk of Famine (Integrated Food Security Phase Classification/Cadre Harmonisé (IPC/CH) Phase 5) or Emergency food insecurity (IPC/CH Phase 4) that is likely to worsen in the coming months.  

**`Conflict, climate, and the economy are the primary drivers of food crops insecurity`**

## **1.2 Problem Statement**

As reported by Agricultural Market Information System (AMIS) in November 2023, the unpredictable weather condition have contributed negative impact on crop yield, which resulted in rising concern on global food crisis as the hunger hotspots are increasing around the world.

**Business Objective:** To solve global food crisis by increase crop’s yield and earlier prediction on weather impact.

**Data Mining Objective:** To predict the crop yield based on weather, rainfall and average temperature by using machine learning algorithm.

# **2. Data Understanding**

## **2.1 Data Collection**

Our project uses an online Database Source called Kaggle : https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset/data

Considerations during data collection:

*   Relevancy of dataset to prediction model
*   Data licensing and usage restrictions where all datasets are publicly available

Data in this site were compiled from the sources of :


*  FAO (Food and Agriculture Organization)  : http://www.fao.org/home/en/
*  World Data Bank : https://data.worldbank.org/



---



First, we will start with extracting the data into our scripts. We will need to follow below steps :



1. Download and upload files listed in the [site](https://www.kaggle.com/datasets/patelris/crop-yield-prediction-dataset/data) to the path

> **`/content/crop_yield_dataset`**



2. Run the scripts below accordingly
"""

from google.colab import drive
drive.mount('/content/drive')

#import CSV files using pandas
import pandas as pd

yield_df_1 = pd.read_csv('/content/yield_df.csv')
yield_df_2 = pd.read_csv('/content/yield.csv')
pesticides_df = pd.read_csv('/content/pesticides.csv')
rainfall_df = pd.read_csv('/content/rainfall.csv')
temp_df = pd.read_csv('/content/temp.csv')

"""Then we will import necessary functions to the script to ensure we are set to go throughout our study."""

# Data
import numpy as np

# Visualization
import matplotlib  #data visualization library
import matplotlib.pyplot as plt  #'pyplot' module to customize plots
import seaborn as sns  #library of data visualization on top of Matplotlib for statistical graphics
matplotlib.rcParams['figure.figsize'] = (10,6)  #To set the default figure size of Matplolib

# Modelling
from sklearn.preprocessing import StandardScaler, MinMaxScaler

"""## **2.2 Describing Data**

From here we will start to have a preliminary overview to describe the data that is available in our dataset. This process will be done as below :


*   Show the first 10 rows to see the overview of the dataset
*   Show the last few rows to see how many rows we have in the dataset
*   Get the info of format types in each columns from the dataset





Since we have 5 different sets of CSV files let's go through the steps one by one.

**1. yield_df.csv**
"""

#To configure to display all columns in just one line.
#By default is true, it will display the dataset based on your screen size, if have many columns and it cannot fit into the screen, the columns will be display at the bottom.
pd.set_option('expand_frame_repr', False)

#To configure to display all the rows. By default, says if have 500 rows of data, it will show only the head & tail of the data
pd.set_option('display.max_rows', None)

#Show the first 10 rows to see the overview of the dataset
yield_df_1.head(10)

#Show the last few rows to see how many rows we have in the dataset
yield_df_1.tail()

#Get the info of format types in each columns from the dataset
yield_df_1.info()

"""**2. yield.csv**"""

#Show the first 10 rows to see the overview of the dataset
yield_df_2.head(10)

#Show the last few rows to see how many rows we have in the dataset
yield_df_2.tail()

#Get the info of format types in each columns from the dataset
yield_df_2.info()

"""**3. pesticides.csv**"""

#Show the first 10 rows to see the overview of the dataset
pesticides_df.head(10)

#Show the last few rows to see how many rows we have in the dataset
pesticides_df.tail()

#Get the info of format types in each columns from the dataset
pesticides_df.info()

"""**4. rainfall.csv**"""

#Show the first 10 rows to see the overview of the dataset
rainfall_df.head(10)

#Show the last few rows to see how many rows we have in the dataset
rainfall_df.tail()

#Get the info of format types in each columns from the dataset
rainfall_df.info()

"""**5. temp.csv**"""

#Show the first 10 rows to see the overview of the dataset
temp_df.head(10)

#Show the last few rows to see how many rows we have in the dataset
temp_df.tail()

#Get the info of format types in each columns from the dataset
temp_df.info()

"""## **2.3 Exploring Data**

From here we will start to explore our data in order to identify our variables. This process will be done as below :


*   Get the count/mean/std/min/max of the dataset wherever necessary
*   Visualize the dataset for a better overview of the distribution

Since we have 5 different sets of CSV files let's go through the steps one by one.

**1. yield_df.csv**
"""

#To get count/mean/std/min/max generally
yield_df_1.describe()

#Visualize the dataset for a better overview
yield_df_1.hist(bins = 25,figsize=(10,10));

# Calculate the mean of each item across different years
mean_per_item = yield_df_1.groupby('Item')['hg/ha_yield'].mean().reset_index()
mean_per_item_sorted = mean_per_item.sort_values(by='hg/ha_yield', ascending=False)

mean_per_item_sorted

# Plotting for bar table
plt.figure(figsize=(5, 3))
plt.barh(mean_per_item_sorted['Item'], mean_per_item_sorted['hg/ha_yield'], color='skyblue')
plt.xlabel('Mean hg/ha_yield')
plt.title('Mean Yield per Item')
plt.gca().invert_yaxis()  # Invert y-axis to display the highest mean at the top
plt.tight_layout()
plt.show()

# Calculate the mean of each item across different years
mean_yield_per_country = yield_df_1.groupby('Area')['hg/ha_yield'].mean().reset_index()
mean_yield_per_country_sorted = mean_yield_per_country.sort_values(by='hg/ha_yield', ascending=False)

mean_yield_per_country_sorted

# Plotting for bar table
plt.figure(figsize=(15, 5))
plt.bar(mean_yield_per_country_sorted['Area'], mean_yield_per_country_sorted['hg/ha_yield'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Yield (hg/h)')
plt.title('Mean Yield per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

# Calculate the mean of rainfall across different countries
meanrainfall_per_country = yield_df_1.groupby('Area')['average_rain_fall_mm_per_year'].mean().reset_index()
meanrainfall_per_country_sorted = meanrainfall_per_country.sort_values(by='average_rain_fall_mm_per_year', ascending=False)

meanrainfall_per_country_sorted

# Plotting for bar table
plt.figure(figsize=(15, 5))
plt.bar(meanrainfall_per_country_sorted['Area'], meanrainfall_per_country_sorted['average_rain_fall_mm_per_year'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Rainfall (mm)')
plt.title('Mean Rainfall per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

# Calculate the mean of temperature across different countries
meantemperature_per_country = yield_df_1.groupby('Area')['avg_temp'].mean().reset_index()
meantemperature_per_country_sorted = meantemperature_per_country.sort_values(by='avg_temp', ascending=False)

meantemperature_per_country_sorted

# Plotting for bar table
plt.figure(figsize=(15, 5))
plt.bar(meantemperature_per_country_sorted['Area'], meantemperature_per_country_sorted['avg_temp'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Temperature (Celcius)')
plt.title('Mean Temperature per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

# Calculate the mean of temperature across different countries
medianpesticide_per_country = yield_df_1.groupby('Area')['pesticides_tonnes'].median().reset_index()
medianpesticide_per_country_sorted = medianpesticide_per_country.sort_values(by='pesticides_tonnes', ascending=False)

medianpesticide_per_country_sorted

# Plotting for bar table
plt.figure(figsize=(15, 5))
plt.bar(medianpesticide_per_country_sorted['Area'], medianpesticide_per_country_sorted['pesticides_tonnes'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Pesticide (Tonnes)')
plt.title('Mean Pesticide per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

"""**2. yield.csv**"""

#To get count/mean/std/min/max generally
yield_df_2.describe()

#Visualize the dataset for a better overview
yield_df_2.hist(bins = 25,figsize=(10,10));

# Calculate the mean of each item across different years
mean_per_item_2 = yield_df_2.groupby('Item')['Value'].mean().reset_index()
mean_per_item_sorted_2 = mean_per_item_2.sort_values(by='Value', ascending=False)

mean_per_item_sorted_2

# Plotting for bar table
plt.figure(figsize=(5, 3))
plt.barh(mean_per_item_sorted_2['Item'], mean_per_item_sorted_2['Value'], color='skyblue')
plt.xlabel('Mean hg/ha_yield')
plt.title('Mean Yield per Item')
plt.gca().invert_yaxis()  # Invert y-axis to display the highest mean at the top
plt.tight_layout()
plt.show()

# Calculate the mean of each yield across different countries
mean_yield_per_country_2 = yield_df_2.groupby('Area')['Value'].mean().reset_index()
mean_yield_per_country_2 = mean_yield_per_country_2.sort_values(by='Value', ascending=False)

mean_yield_per_country_2

# Plotting for bar table
plt.figure(figsize=(30, 10))
plt.bar(mean_yield_per_country_2['Area'], mean_yield_per_country_2['Value'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Yield (hg/h)')
plt.title('Mean Yield per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

"""**3. pesticides.csv**"""

#To get count/mean/std/min/max
pesticides_df.describe()

#Visualize the dataset for a better overview
pesticides_df.hist(bins = 25,figsize=(10,3));

# Calculate the median of pesticides used across different countries
median_pesticides_per_country_2 = pesticides_df.groupby('Area')['Value'].median().reset_index()
median_pesticides_per_country_2_sorted = median_pesticides_per_country_2.sort_values(by='Value', ascending=False)

median_pesticides_per_country_2_sorted

# Plotting for bar table
top_50_countries = median_pesticides_per_country_2_sorted.head(50)

plt.figure(figsize=(15, 5))  # Adjust figure size for better visualization
plt.bar(top_50_countries['Area'], top_50_countries['Value'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Median Pesticides (ton)')
plt.title('Median Pesticides per Country (Top 50)')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""**4. rainfall.csv**"""

#To get count/mean/std/min/max
rainfall_df.describe()

#Visualize the dataset for a better overview
rainfall_df.hist(bins = 25,figsize=(5,3));

# Calculate the mean of rainfall across different countries
mean_rainfall_per_country_2 = yield_df_2.groupby('Area')['Value'].mean().reset_index()
mean_rainfall_per_country_2_sorted = mean_yield_per_country_2.sort_values(by='Value', ascending=False)

mean_rainfall_per_country_2_sorted

# Plotting for bar table
plt.figure(figsize=(30, 10))
plt.bar(mean_rainfall_per_country_2_sorted['Area'], mean_rainfall_per_country_2_sorted['Value'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Rainfall (mm)')
plt.title('Mean Rainfall per Country')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

"""**5. temp.csv**"""

#To get count/mean/std/min/max
temp_df.describe()

#Visualize the dataset for a better overview
temp_df.hist(bins = 25,figsize=(10,3));

# Calculate the mean of rainfall across different countries
mean_temp_per_country = temp_df.groupby('country')['avg_temp'].mean().reset_index()
mean_temp_per_country_sorted = mean_temp_per_country.sort_values(by='avg_temp', ascending=False)

mean_temp_per_country_sorted

# Plotting for bar table
plt.figure(figsize=(30, 10))
plt.bar(mean_temp_per_country_sorted['country'], mean_temp_per_country_sorted['avg_temp'], color='skyblue')
plt.xlabel('Country')
plt.ylabel('Mean Temperature (°C)')
plt.title('Mean Temperature per Country (Descending Order)')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed
plt.tight_layout()
plt.show()

"""## **2.4 Verify Quality**

From here we will start to have a preliminary overview of how our datasets look like. This process will be done as below :


*   Examine missing values
*   Examine duplicated rows

Since we have 5 different sets of CSV files let's go through the steps one by one.

**1. yield_df.csv**
"""

#Examine missing values
yield_df_1.isnull().sum()

#Examine duplicated rows
yield_df_1.duplicated().sum()

"""**2. yield.csv**"""

#Examine missing values
yield_df_2.isnull().sum()

#Examine duplicated rows
yield_df_2.duplicated().sum()

"""**3. pesticides.csv**"""

#Examine missing values
pesticides_df.isnull().sum()

#Examine duplicated rows
pesticides_df.duplicated().sum()

"""**4. rainfall.csv**"""

#Examine missing values
rainfall_df.isnull().sum()

#Examine duplicated rows
rainfall_df.duplicated().sum()

"""**5. temp.csv**"""

#Examine missing values
temp_df.isnull().sum()

#Examine duplicated rows
temp_df.duplicated().sum()

"""## **2.5 Conclusion**


After going through the processes, few key takeaways that we can get from the dataset available :


1. Dependent variable can be identified as the yield field of the dataset. However this field has duplication in 2 of the files (yield_df.csv & yield.csv) hence data merging is necessary. We will need to merge these files and eliminate any duplicative columns or data.
2. There are abundance of independent variables available in the dataset that we can explore ie. areas, product items, temperature etc.  However, narrowing down the variables and only select necessary data would be advisable to focus on our project's objective.
3.   Abundance of columns that are deemed unnecessary for the study are visible ie. Unnamed, Area Code, Domain, Domain code, Element Code, Element etc.
4.   Missing values and duplicated data are prominent especially for rainfall.csv and temp.csv files
5.   Data merging, filteration and cleanups are necessary for us to go further on our study

# **3. Data preparation**

## **3.1 Data cleaning**

**1. yield.csv**

- remove unnecessary columns like Area Code, Domain, Domain code, Element
Code, Element, Item Code, Year Code
- rename Value to hg/ha_yield column
"""

#View 1st 5 rows record of yield.csv
yield_df_2.head()

# rename Value to hg/ha_yield columns.
yield_df_2 = yield_df_2.rename(index=str, columns={"Value": "hg/ha_yield"})
yield_df_2.head()

#removal of unnecessary coloumns like Area Code, Domain, Item Code, etc.
yield_df_2 = yield_df_2.drop(['Domain Code','Domain','Area Code','Element Code','Element','Item Code','Year Code','Unit',], axis=1)
yield_df_2.head()

yield_df_2.info()

yield_df_2['Item'].value_counts()

# Dropping 'Plantains and others'as it may be any number of distinct crops
yield_df_2 = yield_df_2[yield_df_2['Item'] != 'Plantains and others']
yield_df_2['Item'].value_counts()

"""**2. rainfall.csv**
- Drop empty rows from dataset
"""

#View 1st 5 rows record of rainfall.csv
rainfall_df.head()

# check data types
rainfall_df.info()

# convert average_rain_fall_mm_per_year from object to float
rainfall_df['average_rain_fall_mm_per_year'] = pd.to_numeric(rainfall_df['average_rain_fall_mm_per_year'],errors = 'coerce')

#change Area column name without spacing
rainfall_df = rainfall_df.rename(index=str, columns={" Area": 'Area'})

#droping any empty rows from dataset
rainfall_df = rainfall_df.dropna()
rainfall_df.info()

rainfall_df.head()

"""**3. pesticide.csv**
- Rename Values columns to pesticides_tonnes
- Drop unnecessary columns
"""

pesticides_df.head()

pesticides_df = pesticides_df.rename(index=str, columns={"Value": "pesticides_tonnes"})

pesticides_df = pesticides_df.drop(['Element','Domain','Unit','Item'], axis=1)
pesticides_df.head()

pesticides_df.info()

"""**4. temp.csv**
- Rename year & country columns to Year & Area
- Drop empty rows
"""

temp_df.head()

temp_df = temp_df.rename(index=str, columns={"year": "Year", "country": "Area"})
temp_df= temp_df.dropna()
temp_df.head()

temp_df.info()

"""## **3.2 Merge dataframe**"""

# merge yield dataframe with rainfall dataframe by year and area columns , forming total yield dataframe
totalyield_df = pd.merge(yield_df_2, rainfall_df, on=['Year','Area'])
#we view the final shape of the dataframe
totalyield_df.shape

#1st 5 records row of totalyield_df
totalyield_df.head()

# merge Pesticides dataframe with total yield dataframe
totalyield_df = pd.merge(totalyield_df,pesticides_df, on=['Year','Area'])
totalyield_df .shape

#1st 5 records row of totalyield_df
totalyield_df .head()

# merge Temperature dataframe with total yield dataframe
totalyield_df = pd.merge(totalyield_df,temp_df, on=['Area','Year'])
#1st 5 records row of totalyield_df
totalyield_df.head()

totalyield_df.isnull().sum()

"""**totalyield_df is the final obtained dataframe without empty rows.**"""

totalyield_df.info()

"""## **3.3 Outlier**"""

#summary
totalyield_df.describe()

"""**High variance in the values for each columns in total yield dataframe**



"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize = (10,10))

plt.subplot(3,2,1)
sns.boxplot(data= totalyield_df['Year'])
plt.title('Year')


plt.subplot(3,2,2)
sns.boxplot(data= totalyield_df['hg/ha_yield'])
plt.title('hg/ha_yield')

plt.subplot(3,2,3)
sns.boxplot(data= totalyield_df['average_rain_fall_mm_per_year'])
plt.title('average_rain_fall_mm_per_year')

plt.subplot(3,2,4)
sns.boxplot(data= totalyield_df['pesticides_tonnes'])
plt.title('pesticides_tonnes')

plt.subplot(3,2,5)
sns.boxplot(data= totalyield_df['avg_temp'])
plt.title('avg_temp')

plt.show()

"""*   rainfall: Most rainfall is between 500-1500
*   avg_temp: Most average temperatures is around 25. *hg/ha yield: There is a vast majority of yield production around 0.
*   pesticides_tonnes: The majority of used pesticides is little to zero
*   **There are large outliers in the hg/ha_yield and pesticides_tonnes columns.**
"""

# Plot histograms for each variable
totalyield_df.hist(bins=20, figsize=(10, 8))
plt.suptitle('Histograms of Variables', x=0.5, y=1.02, ha='center', fontsize='large')
plt.show()

# Plot histograms only for 'hg/ha_yield' and 'pesticides_tonnes'
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 2)
plt.hist(totalyield_df['pesticides_tonnes'], bins=50, color='blue', alpha=0.7)
plt.title('Histogram of pesticides_tonnes')
plt.xlabel('Pesticides Tonnes')
plt.ylabel('Frequency')

plt.subplot(1, 2, 1)
plt.hist(totalyield_df['hg/ha_yield'], bins=50, color='green', alpha=0.7)
plt.title('Histogram of hg/ha_yield')
plt.xlabel('hg/ha Yield')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Filter based on 'pesticides_tonnes'( keeping only 95th quantile)
pesticides_subset = totalyield_df['pesticides_tonnes'][totalyield_df['pesticides_tonnes'] <= totalyield_df['pesticides_tonnes'].quantile(0.95)]
# Filter based on 'hg/ha_yield' ( keeping only 95th quantile)
yield_subset = totalyield_df['hg/ha_yield'][totalyield_df['hg/ha_yield'] <= totalyield_df['hg/ha_yield'].quantile(0.95)]

# Plot histograms for 'hg/ha_yield' and 'pesticides_tonnes'(To check if the data distribution is the same after keeping only 95th quantile)
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 2)
plt.hist(pesticides_subset, bins=50, color='blue', alpha=0.7)
plt.title('Histogram of pesticides_tonnes')
plt.xlabel('Pesticides Tonnes')
plt.ylabel('Frequency')

plt.subplot(1, 2, 1)
plt.hist(yield_subset, bins=50, color='green', alpha=0.7)
plt.title('Histogram of hg/ha_yield')
plt.xlabel('hg/ha Yield')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Same data distrubtion after keeping only 95th quantile**"""

#To drop only the upper 5% data points to prevent high variance in the data set.
totalyield_df = totalyield_df[(totalyield_df['pesticides_tonnes'] <= totalyield_df['pesticides_tonnes'].quantile(0.95)) &
                              (totalyield_df['hg/ha_yield'] <= totalyield_df['hg/ha_yield'].quantile(0.95))]

"""## **3.4 Data Exploration**

"""

#number of records for each column , group by iteam
totalyield_df.groupby('Item').count()

#number of country (area)
country_counts=totalyield_df['Area'].nunique()
print(f'Number of countries: {country_counts}')

# Count the number of countries with less than 100 records
country_counts1 = totalyield_df['Area'].value_counts()
countries_less_than_100 = (country_counts1 < 100).sum()
print(f'Number of countries with less than 100 records: {countries_less_than_100}')

# Remove countries with less than 100 record
country_counts =totalyield_df['Area'].value_counts()
countries_to_drop = country_counts[country_counts < 100].index.tolist()
df_filtered = totalyield_df[~totalyield_df['Area'].isin(countries_to_drop)]
totalyield_df_2= df_filtered.reset_index(drop=True)

#check for corelation
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt

datacorr=totalyield_df_2.copy()

from sklearn.preprocessing import LabelEncoder
categorical_columns = datacorr.select_dtypes(include=['object']).columns.tolist()
label_encoder = LabelEncoder()
for column in categorical_columns:
    datacorr[column] = label_encoder.fit_transform(datacorr[column])

sns.heatmap(datacorr.corr() , annot= True , cmap='PuOr')
plt.title('Heatmap')

"""

*   There are correlation between Area and pesticides_tonnes, followed by Area and average rainfall
*   There are correlation between Item and hg/ha_yield


"""

# Select relevant columns
selected_columns = ['Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']
selected_df = totalyield_df[selected_columns]

# Set 'Year' as the index
selected_df.set_index('Year', inplace=True)

# Plot line charts for each variable
plt.figure(figsize=(12, 8))

# Line plot for 'hg/ha_yield'
plt.subplot(2, 2, 1)
sns.lineplot(data=selected_df['hg/ha_yield'])
plt.title('hg/ha_yield over Time')
# Line plot for 'average_rain_fall_mm_per_year'
plt.subplot(2, 2, 2)
sns.lineplot(data=selected_df['average_rain_fall_mm_per_year'])
plt.title('Average Rainfall over Time')
# Line plot for 'pesticides_tonnes'
plt.subplot(2, 2, 3)
sns.lineplot(data=selected_df['pesticides_tonnes'])
plt.title('Pesticides Tonnes over Time')
# Line plot for 'avg_temp'
plt.subplot(2, 2, 4)
sns.lineplot(data=selected_df['avg_temp'])
plt.title('Average Temperature over Time')

plt.tight_layout()
plt.show()

"""*   The yield and pesticides used increases graduallly over time"""

# Group by 'Item' and 'Area', sum the 'hg/ha_yield', then select the top 20
top_20_items_areas = totalyield_df.groupby(['Item', 'Area'], sort=True)['hg/ha_yield'].sum().nlargest(20)

# Reshape the data for plotting
pivot_table = top_20_items_areas.unstack('Item', fill_value=0)

# Sort the columns in descending order based on the sum of each column
pivot_table = pivot_table[pivot_table.sum().sort_values(ascending=False).index]

# Plot a grouped bar chart
plt.figure(figsize=(14, 8))
pivot_table.plot(kind='bar', stacked=True, colormap='viridis')
plt.title('Top 20 Total Yields by Item and Area')
plt.xlabel('Area')
plt.ylabel('Total Yield (hg/ha)')
plt.legend(title='Item', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""**India is the country producing the highest yield out of 71 country, producing cassava,sweet potates and potates**"""

# Group by 'Item' and 'Area', sum the 'hg/ha_yield', then select the top 20
top_20_items = totalyield_df.groupby('Item', sort=True)['hg/ha_yield'].sum().nlargest(20)

# Plot a bar chart for 'Item' and 'hg/ha_yield'
plt.figure(figsize=(14, 8))
top_20_items.plot(kind='bar', color='skyblue')
plt.title('Top 20 Total Yields by Item')
plt.xlabel('Item')
plt.ylabel('Total Yield (hg/ha)')
plt.show()

"""**Potatoes are the crops with the highest yield worldwide from 1990 to 2013**"""

import seaborn as sns  # Import seaborn for color palettes

# Group by 'Area', calculate the sum of 'average_rain_fall_mm_per_year' for each area
top_20_areas_rainfall = totalyield_df.groupby('Area')['average_rain_fall_mm_per_year'].sum().nlargest(20)

plot_df = pd.DataFrame({'Area': top_20_areas_rainfall.index, 'Total Rainfall (mm per year)': top_20_areas_rainfall.values})

sns.set_palette('husl')

# Plot a bar chart
plt.figure(figsize=(14, 8))
plt.bar(plot_df['Area'], plot_df['Total Rainfall (mm per year)'])
plt.title('Top 20 Total Rainfall by Area')
plt.xlabel('Area')
plt.ylabel('Total Rainfall (mm per year)')
plt.xticks(rotation=45, ha='right')
plt.ticklabel_format(style='plain', axis='y')
plt.show()

"""
**Top 3 country with the highest amount of rainfall are India, Brazil and Indonesia**"""

import plotly.express as px
areas_of_interest = ['Brazil', 'India', 'Indonesia']
# Filter DataFrame for the specified areas
df_subset = totalyield_df[totalyield_df['Area'].isin(areas_of_interest)]

# Create scatter plot
scatter = px.scatter(df_subset, x='hg/ha_yield', y='average_rain_fall_mm_per_year', color='Area',
                     color_discrete_sequence=px.colors.qualitative.Set1, size_max=12)
scatter.update_layout(title=f'Scatter Plots for {", ".join(areas_of_interest)}',
                      xaxis_title='hg/ha_yield', yaxis_title='average_rain_fall_mm_per_year')
scatter.update_xaxes(tickmode='linear', tick0=0, dtick=5000)  # You can adjust the dtick value as needed

scatter.show()

"""
*   Among Top 3 country with the highest amount of rainfallList, India has the highest yield production is 385k hg/ha


"""

# Group by 'Area', calculate the sum of 'pesticides_tonnes for each area
top_10_areas_pesticides = totalyield_df.groupby('Area')['pesticides_tonnes'].sum().nlargest(10)

plot_df = pd.DataFrame({'Area': top_10_areas_pesticides.index, 'pesticides_tonnes': top_10_areas_pesticides.values})

sns.set_palette('husl')

# Plot a bar chart
plt.figure(figsize=(14, 8))
plt.bar(plot_df['Area'], plot_df['pesticides_tonnes'])
plt.title('Top 10 Pesticides used by Area')
plt.xlabel('Area')
plt.ylabel('pesticides_tonnes')
plt.xticks(rotation=45, ha='right')
plt.ticklabel_format(style='plain', axis='y')
plt.show()

"""**Brazil used the highest amount of pesticides**"""

import plotly.express as px

areas_of_interest = ['Brazil', 'India', 'Japan', 'Mexico', 'Australia', 'Italy', 'Argentina', 'Canada', 'Turkey', 'Colombia']

# Filter DataFrame for the specified areas
df_subset = totalyield_df[totalyield_df['Area'].isin(areas_of_interest)]

# Create scatter plot
scatter = px.scatter(df_subset, x='hg/ha_yield', y='pesticides_tonnes', color='Area',
                     color_discrete_sequence=px.colors.qualitative.Set1, size_max=12)
scatter.update_layout(title=f'Scatter Plots for {", ".join(areas_of_interest)}',
                      xaxis_title='hg/ha_yield', yaxis_title='pesticides_tonnes')

# Show the scatter plot
scatter.show()

"""Do pesticides affect yield production?
*   Among the Top 10 country in using pesticides, most of the country with yield production up to 400k hg/ha
*   Brazil used relatively high amount of pesticides with yield production up to 300k hg/ha

# **4. Data preprocessing**

Data preprocessing including to change the categorical value to numerical value and there're 3 methods to do so which are:
*   Label Encoding
*   One Hot Encoding
*   **Target Encoding**

The columns that required for pre-processing are Area and Item. Both are categorical value and it's not ordinal.

**Since it's not ordinal, Label Encoding is not suitable.**

We can't use One Hot Encoding also because:

For Area, we have a total of 101 unique Area. This will create additional 101 columns.

For Item, we have a total of 10 unique Item. This will create additional 10 columns.

Total additional columns will be 101 + 10 = 111 columns.

An increase in the dimensionality of the dataset causes curses of dimensionality, which leads to the problem of parallelism and multicollinearity.

To overcome this, there's a simple solution that can reduce the dimension which is to take only the top 5 categories that have highest frequency and these 5 categories are make up of more than 85% of the whole dataset.

However, in our dataset, the top 5 areas that have highest frequency are India 4048, Brazil 2277, Mexico 1472, Pakistan 1449, Japan 966. The other areas also have similar frequency count. Hence the top 5 categories are not make up of more than 85% of the whole dataset. Same also for column Item.

**Hence, One Hot Encoding is not suitable as well.**
"""

#check total unique count of Area & Item
unique_countA = totalyield_df['Area'].nunique()
unique_countB = totalyield_df['Item'].nunique()
print(f"Total unique count in 'Area': {unique_countA}")
print(f"Total unique count in 'Item': {unique_countB}")

# Counting by Area
totalyield_df.Area.value_counts()

# Counting by Item
totalyield_df.Item.value_counts()

"""**Target encoding is replacing a categorical feature with average target value of all data points belonging to the category.**

**Target Encoding is suitable for our case.**
"""

#install package
!pip install category_encoders

# use target encoding
import category_encoders as ce
encoder=ce.TargetEncoder(cols=['Area','Item'])
data_en=encoder.fit_transform(totalyield_df[['Area','Item']],totalyield_df['hg/ha_yield'])
df_new = totalyield_df.drop(columns=['Area', 'Item']).join(data_en)
df_new.head(5)

"""There's no need to do smoothing after the Target Encoding because this technique is particularly useful to handle situations when there are very few datapoints for some of the categories. In our case, datapoint is similarly many for every categories."""

plt.figure(figsize = (10,10))

plt.subplot(3,2,1)
sns.boxplot(data= df_new['Year'])
plt.title('Year')


plt.subplot(3,2,2)
sns.boxplot(data= df_new['hg/ha_yield'])
plt.title('hg/ha_yield')

plt.subplot(3,2,3)
sns.boxplot(data= df_new['average_rain_fall_mm_per_year'])
plt.title('average_rain_fall_mm_per_year')

plt.subplot(3,2,4)
sns.boxplot(data= df_new['pesticides_tonnes'])
plt.title('pesticides_tonnes')

plt.subplot(3,2,5)
sns.boxplot(data= df_new['avg_temp'])
plt.title('avg_temp')

plt.show()

"""# **5. Data Modelling**

## **5.1 Preparing Train and Test data**
Divide the dataset into training and test sets. The training set is used to train the model and the test set is used to evaluate the model's performance.
"""

from sklearn.metrics import root_mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import classification_report, confusion_matrix

X, y = df_new.drop(labels='hg/ha_yield', axis=1), df_new['hg/ha_yield']

#Will split to train set and test set to 70 30 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""## **5.2 Modelling**
To predict the crop yield, the suitable algorithm is supervised learning: regression. The models belonging to this category are:
1.   Linear Regression
2.   Decision Tree
3.   Random Forest
"""

results = []

models = [
    ('Linear Regression', LinearRegression()),
    #random_state 42 is set so no matter how many times you execute your code the result would be the same
    ('Decision Tree',DecisionTreeRegressor(random_state=42)),
    ('Random Forest', RandomForestRegressor(random_state=42))
    ]

# print prediction results
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    MSE = mean_squared_error(y_test, y_pred)
    RMSE = root_mean_squared_error(y_test, y_pred)
    R2_score = r2_score(y_test, y_pred)
    results.append((name, R2_score, MSE, RMSE))
    acc = (model.score(X_train , y_train)*100)
    print(f'The accuracy of the {name} Model Train is {acc:.2f}')
    acc =(model.score(X_test , y_test)*100)
    print(f'The accuracy of the {name} Model Test is {acc:.2f}')
    plt.scatter(y_test, y_pred,s=10,color='#B4D4FF')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title(f' {name} Evaluation')
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='#176B87', linewidth = 2)
    plt.show()

dff = pd.DataFrame(results, columns=['Model', 'R2_score', 'MSE', 'RMSE'])
df_styled_best = dff.style.highlight_max(subset=['R2_score'], color='#C3E2C2').highlight_min(subset=['MSE'], color='#C3E2C2').highlight_max(subset=['MSE'], color='#FFC0D9').highlight_min(subset=['RMSE'], color='#C3E2C2').highlight_max(subset=['RMSE'], color='#FFC0D9').highlight_min(subset=['R2_score'], color='#FFC0D9')

display(df_styled_best)

"""The model that have the best performance is Random Forest Regressor with an R2 score of 0.981266.

## **5.3 Hyperparameter Tunning**
"""

# defining parameter range
model_params={
'Linear Regression':{
	'model': LinearRegression(),
	'params':{
    'positive':[True, False]
	}
},
'Decision Tree':{
	'model': DecisionTreeRegressor(),
	'params':{
  'max_depth':[None, 5,10]
	}
},
'Random Forest':{
	'model': RandomForestRegressor(),
	'params':{
	'n_estimators':[70,100,120], #default=100 : number of trees you want to build. Higher number of trees give you better performance but makes your code slower.
	#max_features - default is Auto/None : This will simply take all the features which make sense in every tree.
	'min_samples_leaf':[1,20, 50] #default=1 : Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data
	}
}
}

scores = []
for model_name, mp in model_params.items():
    # fitting the model for grid search
    grid = GridSearchCV(mp['model'],mp['params'], cv=5, return_train_score=False, refit = True, verbose = 3,n_jobs=-1)
    grid.fit(X_train, y_train)
    scores.append({
		'model':model_name,
		'best_score':grid.best_score_,
		'best_params':grid.best_params_
		})

# print best parameter after tuning
df=pd.DataFrame(scores, columns=['model','best_score','best_params'])
df

"""Even though after the hyperparameter tuning, Random Forest R2 score 0.978728 is a bit lower than before tuning 0.981266, however, it's still the best performance among 3 models.

Generally, Random Forest model is preferred due to its strong generalization and performance, allowing for efficient prediction on new data.

## **5.4 Best Model: Predicted Vs Actual**
"""

best_model=RandomForestRegressor(random_state=42)
best_model.fit(X_train,y_train)
y_pred=best_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)

#plotting the results of our model, against the original results
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.7, color='#E0AED0', label='Predicted')
sns.scatterplot(x=y_test, y=y_test, alpha=0.7, color='#756AB6', label='Actual')
plt.xlabel("Actual Values (y_test)")
plt.ylabel("Predicted Values (y_pred)")
plt.title("Actual vs. Predicted Values")
plt.grid(True)
plt.legend()
plt.show()

"""# **6. Result Evaluation & Interpretation**

In order to evaluate the predictive performance of the machine learning models, the **Residual Plots** (scatter plot of residuals against predicted values) is plotted to identify the distribution and changing spread of residuals.
As a good residual plot,
1.   Residuals should be normally distributed, and there should be no patterns or trends.
2.   The result is not heteroscedasticity, meaning that the spread of residuals remains relatively constant across different levels of the fitted values.

Also, the metrics such as **Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or R-squared** are calculated to assess the performance of the regression models.

*   Mean Absolute Error (MAE): average of the absolute differences between predicted and actual values.
*   Mean Squared Error (MSE): average squared difference between predicted and actual values. Higher weight represents larger error.
*   Root Mean Squared Error (RMSE): square root of the MSE and provides an interpretable measure in the same unit as the target variable.
*   R-squared (R2): proportion of the variance in the dependent variable, range from 0 to 1, where 1 indicates a perfect fit.
"""

# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)

# Decision Tree
tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)

# Random Forest
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Gradient Boosting
#gb_model = GradientBoostingRegressor(random_state=42)
#gb_model.fit(X_train, y_train)
#y_pred_gb = gb_model.predict(X_test)

# Evaluation and Residual Plots
models = [linear_model, tree_model, rf_model]#, gb_model]
model_names = ['Linear Regression', 'Decision Tree', 'Random Forest']#, 'Gradient Boosting']

for model, model_name in zip(models, model_names):
    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Residual plot
    residuals = y_test - y_pred
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_pred, y=residuals)
    plt.title(f'Residual Plot - {model_name}')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.show()

    # Display results
    print(f'Model: {model_name}')
    print(f'Mean Absolute Error (MAE): {mae:.4f}')
    print(f'Mean Squared Error (MSE): {mse:.4f}')
    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')
    print(f'R-squared (R2): {r2:.4f}')

"""After calculated the evaluation matrix, the best model to suggested to proceed for pratical purpose is **Random Forest** , due to higher reliability as good residual plot result presented and high accuracy (R2) as 98.13% as compared with other models.

**For result interpretation**, variable importance stability is determined to interpret the consistency and robustness of the model's assessment of feature importance.
It commonly used for understanding which features contribute consistently to the model's prediction results and perform with high stability.

Before conducting any evaluation analysis, the skewness of the data are considering before decided the method of model evaluation.

Generally, for skewness:
*   Close to 0 : the distribution is approximately symmetric.
*   Between -0.5 and 0.5: mild or moderate skewness.
*   Between -1 and -0.5 (or 0.5 and 1: moderate skewness.
*  Less than -1 or greater than 1: highly skewed.
"""

from scipy.stats import skew

# Calculate skewness
skewness_results = pd.DataFrame({'Variable': df_new.columns, 'Skewness': [skew(df_new[col]) for col in df_new.columns]})

skewness_results['Interpretation'] = skewness_results['Skewness'].apply(
    lambda x: "Approximately symmetric" if x == 0 else "Right-skewed" if x > 0 else "Left-skewed"
)
print(skewness_results)

"""Since most of the data is highly to moderately skewed, the **Random Subsampling technique** is used to estimate the sampling distrubution of the dataset by repeatedly resampling with replacement from the data.

This method is useful when dealing with skewed data and large dataset to reduce computational complexity.
"""

#Initial dataset
df_new.head(5)

# Sample data
data = {
    'Year': np.random.rand(100),
    'hg/ha_yield': np.random.rand(100),
    'average_rain_fall_mm_per_year': np.random.rand(100),
    'pesticides_tonnes': np.random.rand(100),
    'avg_temp': np.random.rand(100),
    'Area': np.random.rand(100),
    'Item': np.random.rand(100),
    'Target': np.random.rand(100)
}

df = pd.DataFrame(data)
X_train = df[['Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'Area', 'Item']]
y_train = df['Target']

# Feature names from your series
specified_feature_names = ['Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'Area', 'Item']

num_subsamples_to_test = [10, 50, 100]
results = []

for num_subsamples in num_subsamples_to_test:
    variable_importance_subsamples = []

    # Perform Random Subsampling
    for i in range(num_subsamples):
        # Random subsampling
        subsample_indices = np.random.choice(len(X_train), size=len(X_train), replace=False)
        X_subsample = X_train.iloc[subsample_indices]
        y_subsample = y_train.iloc[subsample_indices]

        # Use the pre-trained model on the subsample
        model = RandomForestRegressor(random_state=42)  # Initialize your model here
        model.fit(X_subsample, y_subsample)
        variable_importance_subsamples.append(model.feature_importances_)

    # Calculate Stability Metrics
    mean_importance = np.mean(variable_importance_subsamples, axis=0)
    mad = np.mean(np.abs(variable_importance_subsamples - mean_importance), axis=0)

    # Append results to the list with specified feature names
    for feature_name, mean_imp, mad_value in zip(specified_feature_names, mean_importance, mad):
        results.append({
            'Number of Random Subsamples': num_subsamples,
            'Feature': feature_name,
            'Mean Importance': mean_imp,
            'MAD': mad_value
        })

# Create a DataFrame from the results
df_results = pd.DataFrame(results)

# Sort the DataFrame by specified feature names
df_results = df_results.sort_values(by=['Feature', 'Number of Random Subsamples'])

# Display the DataFrame
print(df_results)

"""Mean Importance and Mean Absolute Deviation (MAD) are calculated to evaluate the feature importance values and variability/dispersion across the random subsamples.

There are some conclusion can be make for result interpretation:
*   Low MAD value are obtained through different random subsamples. This indicated the stability of feature importance estimates across different random subsamples are **ralatively stable and consistent**.
*   The **highest mean importance variable** of this project is **'average_rain_fall_mm_per_year** as it consistently has higher mean importance values compared to other features, the **second highest is 'avg_temp'**.

Using this information, the impact of variables can be explained to stakeholders for future improvement in crop yield optimization through prioritize the control or planning in higher importance variable.

# **7. Member's Contribution**


1. Business Understanding : LAW JIA JIN (22071390)
2. Data Understanding :  NUR SHAFIQAH BINTI MOHAMAD JOHARI (22119564​)
3. Data Preparation : LIM SZE SING (22109557)
4. Data Preprocessing : LIM SZE SING (22109557)
5. Data Modelling : GAN JING WEN (22065433)
6. Result Evaluation & Interpretation : LAW JIA JIN (22071390)
"""